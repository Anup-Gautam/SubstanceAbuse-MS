{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HTIPlahynlud"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "try:\n",
        "    df = pd.read_csv('Drug_Consumption.csv')\n",
        "    display(df.head())\n",
        "    print(df.shape)\n",
        "except FileNotFoundError:\n",
        "    print(\"Error: 'Drug_Consumption.csv' not found. Please ensure the file is in the correct location and accessible.\")\n",
        "except Exception as e:\n",
        "    print(f\"An unexpected error occurred: {e}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# 1. Examine data types and inconsistencies\n",
        "print(df.info())\n",
        "\n",
        "# 2. Descriptive statistics of numerical features\n",
        "print(df.describe())\n",
        "\n",
        "# 3. Distribution of categorical features\n",
        "categorical_cols = ['Gender', 'Education', 'Country', 'Ethnicity']\n",
        "for col in categorical_cols:\n",
        "    print(f\"\\nValue counts for {col}:\\n{df[col].value_counts()}\")\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    df[col].value_counts().plot(kind='bar')\n",
        "    plt.title(f'Distribution of {col}')\n",
        "    plt.xlabel(col)\n",
        "    plt.ylabel('Count')\n",
        "    plt.show()\n",
        "\n",
        "# 4. Correlation between features related to drug usage\n",
        "drug_usage_cols = ['Alcohol', 'Amphet', 'Amyl', 'Benzos', 'Cannabis', 'Coke', 'Crack', 'Ecstasy', 'Heroin', 'Ketamine', 'Legalh', 'LSD', 'Meth', 'Mushrooms', 'Nicotine', 'Semer', 'VSA']\n",
        "correlation_matrix = df[drug_usage_cols].corr()\n",
        "plt.figure(figsize=(12, 10))\n",
        "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=\".2f\")\n",
        "plt.title('Correlation Matrix of Drug Usage')\n",
        "plt.show()\n",
        "\n",
        "# 5. Determine the target variable\n",
        "# Create a composite variable representing overall drug use\n",
        "df['overall_drug_use'] = df[drug_usage_cols].apply(lambda row: sum([int(x[2:]) for x in row]), axis=1)\n",
        "print(f\"\\nValue counts for overall_drug_use:\\n{df['overall_drug_use'].value_counts()}\")\n",
        "plt.figure(figsize=(8, 6))\n",
        "df['overall_drug_use'].value_counts().plot(kind='bar')\n",
        "plt.title('Distribution of Overall Drug Use')\n",
        "plt.xlabel('Overall Drug Use')\n",
        "plt.ylabel('Count')\n",
        "plt.show()\n",
        "\n",
        "# Target variable rationale:\n",
        "# The 'overall_drug_use' score is derived by summing the usage levels of various drugs.  Higher scores indicate higher overall drug consumption. This provides a single metric to predict the likelihood of future drug abuse.\n",
        "\n",
        "# 6. Document findings:\n",
        "# The dataset contains various demographic and drug usage features. 'overall_drug_use' is selected as the target variable, reflecting the overall level of drug consumption.\n",
        "# Potential issues include the coding scheme for drug usage and potential biases in the data collection process. Further investigation into the meaning of each code is necessary."
      ],
      "metadata": {
        "id": "E1ldXxYknszL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Convert drug usage columns to numerical values\n",
        "drug_usage_cols = ['Alcohol', 'Amphet', 'Amyl', 'Benzos', 'Cannabis', 'Coke', 'Crack', 'Ecstasy', 'Heroin', 'Ketamine', 'Legalh', 'LSD', 'Meth', 'Mushrooms', 'Nicotine', 'Semer', 'VSA']\n",
        "for col in drug_usage_cols:\n",
        "    df[col] = df[col].str[2:].astype(int)\n",
        "\n",
        "# Recalculate the correlation matrix\n",
        "correlation_matrix = df[drug_usage_cols].corr()\n",
        "plt.figure(figsize=(12, 10))\n",
        "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=\".2f\")\n",
        "plt.title('Correlation Matrix of Drug Usage')\n",
        "plt.show()\n",
        "\n",
        "# Recalculate overall_drug_use\n",
        "df['overall_drug_use'] = df[drug_usage_cols].sum(axis=1)\n",
        "print(f\"\\nValue counts for overall_drug_use:\\n{df['overall_drug_use'].value_counts()}\")\n",
        "plt.figure(figsize=(8, 6))\n",
        "df['overall_drug_use'].value_counts().plot(kind='bar')\n",
        "plt.title('Distribution of Overall Drug Use')\n",
        "plt.xlabel('Overall Drug Use')\n",
        "plt.ylabel('Count')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "3o01xVPnoJYd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Check data types and missing values\n",
        "print(df.info())\n",
        "\n",
        "# Descriptive statistics for numerical columns\n",
        "print(df.describe())\n",
        "\n",
        "# Value counts and visualizations for categorical columns\n",
        "categorical_cols = ['Gender', 'Education', 'Country', 'Ethnicity']  # Add other categorical columns as needed\n",
        "for col in categorical_cols:\n",
        "    print(f\"\\nValue counts for {col}:\\n{df[col].value_counts()}\")\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    df[col].value_counts().plot(kind='bar')\n",
        "    plt.title(f'Distribution of {col}')\n",
        "    plt.xlabel(col)\n",
        "    plt.ylabel('Count')\n",
        "    plt.show()\n",
        "\n",
        "# Correlation analysis (excluding non-numeric columns)\n",
        "numeric_df = df.select_dtypes(include=['number'])\n",
        "correlation_matrix = numeric_df.corr()\n",
        "plt.figure(figsize=(12, 10))\n",
        "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=\".2f\")\n",
        "plt.title('Correlation Matrix of Numerical Features')\n",
        "plt.show()\n",
        "\n",
        "# Check for inconsistencies (example: unrealistic age values)\n",
        "print(\"\\nAge range:\", df['Age'].min(), df['Age'].max())"
      ],
      "metadata": {
        "id": "hwUBlossoV6A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert 'Age' to numeric (if not already) and handle potential inconsistencies.\n",
        "# It seems 'Age' already is numeric, so no conversion is needed.\n",
        "# Check for unrealistic age values. In the exploration, we found ages from 1 to 5.\n",
        "# These represent categories, not actual ages. We'll leave them as categorical.\n",
        "\n",
        "# Convert drug consumption columns to categorical.\n",
        "drug_columns = ['Alcohol', 'Amphet', 'Amyl', 'Benzos', 'Caff', 'Cannabis', 'Choc', 'Coke', 'Crack', 'Ecstasy', 'Heroin', 'Ketamine', 'Legalh', 'LSD', 'Meth', 'Mushrooms', 'Nicotine', 'Semer', 'VSA']\n",
        "for col in drug_columns:\n",
        "    df[col] = pd.Categorical(df[col])\n",
        "\n",
        "# Convert other categorical columns to categorical data type.\n",
        "categorical_cols = ['Gender', 'Education', 'Country', 'Ethnicity']\n",
        "for col in categorical_cols:\n",
        "    df[col] = pd.Categorical(df[col])\n",
        "\n",
        "# Double check the data types\n",
        "print(df.info())\n",
        "\n",
        "display(df.head())"
      ],
      "metadata": {
        "id": "4nJTexPsqC8d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Create a copy of the original DataFrame to avoid modifying the original data\n",
        "df_encoded = df.copy()\n",
        "\n",
        "# Numerical Encoding of Drug Consumption\n",
        "# Use Label Encoding for drug consumption columns.  CL0 represents non-use.\n",
        "# Higher CL values indicate higher consumption levels. This preserves the ordinal relationship.\n",
        "\n",
        "drug_columns = ['Alcohol', 'Amphet', 'Amyl', 'Benzos', 'Caff', 'Cannabis', 'Choc', 'Coke', 'Crack', 'Ecstasy', 'Heroin', 'Ketamine', 'Legalh', 'LSD', 'Meth', 'Mushrooms', 'Nicotine', 'Semer', 'VSA']\n",
        "for col in drug_columns:\n",
        "    df_encoded[col] = df_encoded[col].astype(str).str.replace('CL', '').astype(int)\n",
        "\n",
        "\n",
        "# Encoding of Categorical Features\n",
        "# One-hot encode 'Gender', 'Education', 'Country', and 'Ethnicity'\n",
        "categorical_cols = ['Gender', 'Education', 'Country', 'Ethnicity']\n",
        "df_encoded = pd.get_dummies(df_encoded, columns=categorical_cols, drop_first=True)\n",
        "\n",
        "# Feature Creation\n",
        "# Total Number of Drugs Used\n",
        "df_encoded['Total_Drugs_Used'] = df_encoded[drug_columns].sum(axis=1)\n",
        "\n",
        "# Risk Level Categorization (Example)\n",
        "# Define thresholds for risk levels based on total drug usage. Adjust these thresholds as needed.\n",
        "def categorize_risk(total_drugs):\n",
        "    if total_drugs <= 10:\n",
        "        return 'low risk'\n",
        "    elif total_drugs <= 25:\n",
        "        return 'medium risk'\n",
        "    else:\n",
        "        return 'high risk'\n",
        "\n",
        "df_encoded['Risk_Level'] = df_encoded['Total_Drugs_Used'].apply(categorize_risk)\n",
        "\n",
        "# Remove any redundant columns (if needed) -  Example, remove the original columns\n",
        "# df_encoded = df_encoded.drop(columns=categorical_cols)\n",
        "\n",
        "display(df_encoded.head())"
      ],
      "metadata": {
        "id": "XXFKfGVHqDTJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Define features (X) and target (y)\n",
        "X = df_encoded.drop('Risk_Level', axis=1)\n",
        "y = df_encoded['Risk_Level']\n",
        "\n",
        "# Initial split into temporary training and combined validation/testing sets\n",
        "X_temp, X_test, y_temp, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, stratify=y, random_state=42\n",
        ")\n",
        "\n",
        "# Split the combined validation/testing set into separate validation and testing sets\n",
        "X_train, X_val, y_train, y_val = train_test_split(\n",
        "    X_temp, y_temp, test_size=0.5, stratify=y_temp, random_state=42\n",
        ")\n",
        "\n",
        "# Display the shapes of the resulting sets to verify the split\n",
        "print(\"X_train shape:\", X_train.shape)\n",
        "print(\"X_val shape:\", X_val.shape)\n",
        "print(\"X_test shape:\", X_test.shape)\n",
        "print(\"y_train shape:\", y_train.shape)\n",
        "print(\"y_val shape:\", y_val.shape)\n",
        "print(\"y_test shape:\", y_test.shape)"
      ],
      "metadata": {
        "id": "ZA6J1zvBqDi8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.preprocessing import PolynomialFeatures, StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "# 1. Polynomial Features\n",
        "numerical_cols = ['Age', 'Alcohol', 'Amphet', 'Amyl', 'Benzos', 'Caff', 'Cannabis', 'Choc', 'Coke', 'Crack', 'Ecstasy', 'Heroin', 'Ketamine', 'Legalh', 'LSD', 'Meth', 'Mushrooms', 'Nicotine', 'Semer', 'VSA', 'Total_Drugs_Used']\n",
        "poly = PolynomialFeatures(degree=2, include_bias=False)\n",
        "X_train_poly = poly.fit_transform(X_train[numerical_cols])\n",
        "X_val_poly = poly.transform(X_val[numerical_cols])\n",
        "\n",
        "# 2. Interaction Features (Example)\n",
        "# Create interaction between 'Age' and 'Total_Drugs_Used'\n",
        "X_train_poly = np.column_stack((X_train_poly, X_train['Age'] * X_train['Total_Drugs_Used']))\n",
        "X_val_poly = np.column_stack((X_val_poly, X_val['Age'] * X_val['Total_Drugs_Used']))\n",
        "\n",
        "\n",
        "# 3. Scaling\n",
        "scaler = StandardScaler()\n",
        "X_train_poly = scaler.fit_transform(X_train_poly)\n",
        "X_val_poly = scaler.transform(X_val_poly)\n",
        "\n",
        "# 4. Dimensionality Reduction (Optional -  uncomment if needed)\n",
        "# pca = PCA(n_components=0.95)  # Keep 95% of variance\n",
        "# X_train_poly = pca.fit_transform(X_train_poly)\n",
        "# X_val_poly = pca.transform(X_val_poly)\n",
        "\n",
        "print(\"X_train_poly shape:\", X_train_poly.shape)\n",
        "print(\"X_val_poly shape:\", X_val_poly.shape)"
      ],
      "metadata": {
        "id": "9D9y1Z5kqI4f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
        "import numpy as np\n",
        "\n",
        "# Initialize the Random Forest Classifier\n",
        "rf_classifier = RandomForestClassifier(random_state=42)\n",
        "\n",
        "# Train the classifier\n",
        "rf_classifier.fit(X_train_poly, y_train)\n",
        "\n",
        "# Make predictions on the validation set\n",
        "y_pred = rf_classifier.predict(X_val_poly)\n",
        "\n",
        "# Evaluate the model\n",
        "accuracy = accuracy_score(y_val, y_pred)\n",
        "precision = precision_score(y_val, y_pred, average='weighted')\n",
        "recall = recall_score(y_val, y_pred, average='weighted')\n",
        "f1 = f1_score(y_val, y_pred, average='weighted')\n",
        "conf_matrix = confusion_matrix(y_val, y_pred)\n",
        "\n",
        "print(f\"Accuracy: {accuracy}\")\n",
        "print(f\"Precision: {precision}\")\n",
        "print(f\"Recall: {recall}\")\n",
        "print(f\"F1-score: {f1}\")\n",
        "print(f\"Confusion Matrix:\\n{conf_matrix}\")"
      ],
      "metadata": {
        "id": "qpAQQn4IqJL8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from sklearn.metrics import make_scorer, f1_score\n",
        "\n",
        "# Define the parameter grid\n",
        "param_grid = {\n",
        "    'n_estimators': [50, 100, 200],\n",
        "    'max_depth': [None, 10, 20, 30],\n",
        "    'min_samples_split': [2, 5, 10],\n",
        "    'min_samples_leaf': [1, 2, 4],\n",
        "    'max_features': ['sqrt', 'log2', None]\n",
        "}\n",
        "\n",
        "# Create the RandomizedSearchCV object\n",
        "f1_scorer = make_scorer(f1_score, average='weighted')\n",
        "random_search = RandomizedSearchCV(\n",
        "    estimator=rf_classifier,\n",
        "    param_distributions=param_grid,\n",
        "    n_iter=10,\n",
        "    scoring=f1_scorer,\n",
        "    cv=5,\n",
        "    n_jobs=-1,\n",
        "    verbose=1,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Fit the random search to the training data\n",
        "random_search.fit(X_train_poly, y_train)\n",
        "\n",
        "# Print the best hyperparameters\n",
        "print(\"Best Hyperparameters:\", random_search.best_params_)\n",
        "\n",
        "# Evaluate the best model on the validation set\n",
        "best_rf_classifier = random_search.best_estimator_\n",
        "y_pred_best = best_rf_classifier.predict(X_val_poly)\n",
        "\n",
        "accuracy_best = accuracy_score(y_val, y_pred_best)\n",
        "precision_best = precision_score(y_val, y_pred_best, average='weighted')\n",
        "recall_best = recall_score(y_val, y_pred_best, average='weighted')\n",
        "f1_best = f1_score(y_val, y_pred_best, average='weighted')\n",
        "conf_matrix_best = confusion_matrix(y_val, y_pred_best)\n",
        "\n",
        "print(f\"Best Model Accuracy: {accuracy_best}\")\n",
        "print(f\"Best Model Precision: {precision_best}\")\n",
        "print(f\"Best Model Recall: {recall_best}\")\n",
        "print(f\"Best Model F1-score: {f1_best}\")\n",
        "print(f\"Best Model Confusion Matrix:\\n{conf_matrix_best}\")"
      ],
      "metadata": {
        "id": "MMTcZ4f2qZMD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
        "\n",
        "# Transform the test data using the same transformers fitted on the training data\n",
        "X_test_poly = poly.transform(X_test[numerical_cols])\n",
        "X_test_poly = np.column_stack((X_test_poly, X_test['Age'] * X_test['Total_Drugs_Used']))\n",
        "X_test_poly = scaler.transform(X_test_poly)\n",
        "\n",
        "# Predict on the test set\n",
        "y_pred_test = best_rf_classifier.predict(X_test_poly)\n",
        "\n",
        "# Evaluate the best model on the test set\n",
        "accuracy_test = accuracy_score(y_test, y_pred_test)\n",
        "precision_test = precision_score(y_test, y_pred_test, average='weighted')\n",
        "recall_test = recall_score(y_test, y_pred_test, average='weighted')\n",
        "f1_test = f1_score(y_test, y_pred_test, average='weighted')\n",
        "conf_matrix_test = confusion_matrix(y_test, y_pred_test)\n",
        "\n",
        "print(f\"Test Set Accuracy: {accuracy_test}\")\n",
        "print(f\"Test Set Precision: {precision_test}\")\n",
        "print(f\"Test Set Recall: {recall_test}\")\n",
        "print(f\"Test Set F1-score: {f1_test}\")\n",
        "print(f\"Test Set Confusion Matrix:\\n{conf_matrix_test}\")"
      ],
      "metadata": {
        "id": "JhhwSsXTqpS-"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}